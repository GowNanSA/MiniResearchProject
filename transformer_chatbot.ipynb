{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8a2835d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\nanda\\anaconda3\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\nanda\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nanda\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\nanda\\anaconda3\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: requests in c:\\users\\nanda\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nanda\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nanda\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nanda\\anaconda3\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nanda\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nanda\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\nanda\\anaconda3\\lib\\site-packages (from transformers) (0.25.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nanda\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nanda\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\nanda\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\nanda\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\nanda\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nanda\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\nanda\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nanda\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import torch \n",
    "!pip install transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "import torch \n",
    "from transformers import pipeline \n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# source https://huggingface.co/docs/transformers/main/en/conversations\n",
    "\n",
    "# https://jasminbharadiya.medium.com/building-a-simple-chatbot-with-python-and-transformers-875aec2f05d8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2efeef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_length):\n",
    "    # Add context to the prompt to guide response generation\n",
    "    contextual_prompt = f\"Respond as a friendly chatbot: {prompt}\"\n",
    "    input_ids = tokenizer.encode(contextual_prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate response with diverse parameters\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length + len(input_ids[0]),  # Ensure it generates enough tokens\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=50256,\n",
    "            temperature=0.9,  # Adjust temperature for more randomness\n",
    "            top_k=50,         # Limit to top k tokens\n",
    "            top_p=0.95,       # Use nucleus sampling\n",
    "            do_sample=True    # Enable sampling\n",
    "        )\n",
    "\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remove the contextual prompt from the response\n",
    "    response = response[len(contextual_prompt):].strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "217401ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using the C++ Standard C# compiler. Add a static class to the top of the class which will be used to build the program.\n",
      "\n",
      "Add a variable to the top of the class which will be used to build the program. Check the file of the code for an error message like \"Unknown class type\", \"Unknown type\", or \"Unknown class name\". You can also use the following functions to check if the code is running (see below for examples):\n",
      "\n",
      "Get the value of\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "prompt = \"Generate me a file of a basic c++ program\"\n",
    "response = generate_response(prompt, max_length=100)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c449f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Taking input\")\n",
    "while True:\n",
    "    user_input = input(\"Prompt \")\n",
    "    if user_input == \"exit\":\n",
    "        print(\"End\")\n",
    "        break\n",
    "\n",
    "    response = generate_response(user_input, max_length = 500)\n",
    "    print(\"Chatbot:\", response)\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a1e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I need to figure out how to train this type of \n",
    "\n",
    "# model on other kinds of data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ed5d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da223573",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
